{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing some libraries\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# importing the dataset\n",
    "dico = pd.read_json(\"dico_features.json\")\n",
    "\n",
    "\n",
    "#shuffling the rows \n",
    "#dico_ = dico.sample(frac=1)\n",
    "\n",
    "#X2 = dico_.iloc[:,0:200]\n",
    "\n",
    "X = dico.drop(['segment'], axis=1).values\n",
    "\n",
    "# this  replaces the NaN data point with 0\n",
    "X = pd.DataFrame(X).fillna(value = 0, ).values\n",
    "\n",
    "\n",
    "y = dico.loc[:,'segment'].values\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "labelencoder_y = LabelEncoder()\n",
    "y = labelencoder_y.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
       "                     weights='uniform')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Fitting classifier to the Training set\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "classifier = KNeighborsClassifier (n_neighbors=5, metric= 'minkowski', p=2 )\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 82,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,  17,   0,   0,   0,   0,   0,   0,   0,   0,   1],\n",
       "       [  0,   0,  51,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   6,  58,   1,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,  93,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,  55,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   4,  30,   1,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   1,   0,  53,   1,   0,   1],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   1,  47,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   1,   1,  58,   1],\n",
       "       [  0,   1,   0,   0,   0,   0,   0,   0,   0,   0, 121]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[604,   0],\n",
       "        [  0,  82]],\n",
       "\n",
       "       [[667,   1],\n",
       "        [  1,  17]],\n",
       "\n",
       "       [[629,   6],\n",
       "        [  0,  51]],\n",
       "\n",
       "       [[621,   0],\n",
       "        [  7,  58]],\n",
       "\n",
       "       [[592,   1],\n",
       "        [  0,  93]],\n",
       "\n",
       "       [[626,   5],\n",
       "        [  0,  55]],\n",
       "\n",
       "       [[651,   0],\n",
       "        [  5,  30]],\n",
       "\n",
       "       [[627,   3],\n",
       "        [  3,  53]],\n",
       "\n",
       "       [[636,   2],\n",
       "        [  1,  47]],\n",
       "\n",
       "       [[625,   0],\n",
       "        [  3,  58]],\n",
       "\n",
       "       [[561,   3],\n",
       "        [  1, 121]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# multilabel_confusion_matrix\n",
    "from sklearn.metrics import multilabel_confusion_matrix as mcm\n",
    "mcm(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9693877551020408"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#Accuracy Score\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.974843975596084"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# explained_variance_score\n",
    "from sklearn.metrics import explained_variance_score as evs\n",
    "evs(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'              precision    recall  f1-score   support\\n\\n           0       1.00      1.00      1.00        82\\n           1       0.94      0.94      0.94        18\\n           2       0.89      1.00      0.94        51\\n           3       1.00      0.89      0.94        65\\n           4       0.99      1.00      0.99        93\\n           5       0.92      1.00      0.96        55\\n           6       1.00      0.86      0.92        35\\n           7       0.95      0.95      0.95        56\\n           8       0.96      0.98      0.97        48\\n           9       1.00      0.95      0.97        61\\n          10       0.98      0.99      0.98       122\\n\\n    accuracy                           0.97       686\\n   macro avg       0.97      0.96      0.96       686\\nweighted avg       0.97      0.97      0.97       686\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Classification report\n",
    "from sklearn.metrics import classification_report\n",
    "c_report = classification_report(y_test, y_pred)\n",
    "c_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
