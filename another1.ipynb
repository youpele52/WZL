{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm # this is progress bar basically\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "from PIL import Image\n",
    "from torch.autograd import Variable\n",
    "import scipy.ndimage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "REBUILD_DATA = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GoodVSCorrupt():\n",
    "    IMG_SIZE = 50\n",
    "    CORRUPT = \"/Users/youpele/Documents/WZL/12032020/corrupt_vs_good_signal/corrupted_data/\"\n",
    "    GOOD = \"/Users/youpele/Documents/WZL/12032020/corrupt_vs_good_signal/good_data/\"\n",
    "    TESTING = \"PetImages/Testing\"\n",
    "    LABELS = {CORRUPT: 0, GOOD: 1}\n",
    "    training_data = []\n",
    "\n",
    "    cor_count = 0\n",
    "    go_count = 0\n",
    "\n",
    "    def make_training_data(self):\n",
    "        for label in self.LABELS:\n",
    "            print(label)\n",
    "            for f in tqdm(os.listdir(label)):\n",
    "                if \"png\" in f:\n",
    "                    try:\n",
    "                        path = os.path.join(label, f)\n",
    "                        img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "                        img = cv2.resize(img, (self.IMG_SIZE, self.IMG_SIZE))\n",
    "                        self.training_data.append([np.array(img), np.eye(2)[self.LABELS[label]]])  # do something like print(np.eye(2)[1]), just makes one_hot \n",
    "                        \n",
    "                        # np.eye(2)[self.LABELS[label]] \n",
    "                        # this line create one hot encode for the classes\n",
    "                        # if its is cat it returns [1,0], and for dog [0,1]\n",
    "                        #print(np.eye(2)[self.LABELS[label]])\n",
    "\n",
    "                        if label == self.CORRUPT:\n",
    "                            self.cor_count += 1\n",
    "                        elif label == self.GOOD:\n",
    "                            self.go_count += 1\n",
    "\n",
    "                    except Exception as e:\n",
    "                        pass\n",
    "                        #print(label, f, str(e))\n",
    "\n",
    "        np.random.shuffle(self.training_data)\n",
    "        np.save(\"training_data_21_03_20.npy\", self.training_data)\n",
    "        print('CORRUPT:',goodvscorrupt.cor_count)\n",
    "        print('GOOD:',goodvscorrupt.go_count)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# creating the model\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 5)\n",
    "        # input = 1\n",
    "        # output = 32  convolutional features\n",
    "        # kernel = 5, this is going to make a 5 by 5 kernel/window as it rolls/slides over our data to features\n",
    "        self.conv2 = nn.Conv2d(32, 64, 5)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 5)\n",
    "        #self.conv4 = nn.Conv2d(128, 256, 5)\n",
    "        # nn.Conv3d are used for scans or models\n",
    "\n",
    "        x = torch.randn(50,50).view(-1,1,50,50)\n",
    "        # 50*50 is the size of the image that we have resized earlier\n",
    "        # view to flatten the image, \n",
    "        # (-1, to prepare to accept any feature we have\n",
    "        # 1, a mirror of the input in the first conv1 layer\n",
    "        # 50*50, same as before. )\n",
    "        \n",
    "        self._to_linear = None\n",
    "        \n",
    "        self.convs(x)\n",
    "\n",
    "        self.fc1 = nn.Linear(self._to_linear, 512)\n",
    "        # the 512 is random guessed number\n",
    "        # we dont know the number that is supposed to be in self._to_linear position \n",
    "        # thus theres  a lil scrip forward that will allow us predict then use it automatically \n",
    "        self.fc2 = nn.Linear(512, 2)\n",
    "        #self.fc3 = nn.Linear(1024, 2)\n",
    "\n",
    "    def convs(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2,2))\n",
    "        # 2 by 2, is the shape of the pooling\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), (2,2))\n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        if self._to_linear is None:\n",
    "            self._to_linear = x[0].shape[0]*x[0].shape[1]*x[0].shape[2]\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.convs(x)\n",
    "        x = x.view(-1, self._to_linear)  #we flatten\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return F.softmax(x, dim=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on the CPU\n"
     ]
    }
   ],
   "source": [
    "# check for gpu \n",
    "        \n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")  # you can continue going on here, like cuda:1 cuda:2....etc. \n",
    "    print(\"Running on the GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Running on the CPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/4043 [00:00<05:12, 12.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/youpele/Documents/WZL/12032020/corrupt_vs_good_signal/corrupted_data/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4043/4043 [04:59<00:00, 13.48it/s]\n",
      "  0%|          | 2/4045 [00:00<05:01, 13.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/youpele/Documents/WZL/12032020/corrupt_vs_good_signal/good_data/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4045/4045 [04:45<00:00, 14.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORRUPT: 4043\n",
      "GOOD: 4045\n"
     ]
    }
   ],
   "source": [
    "\n",
    "net = Net().to(device)\n",
    "\n",
    "if REBUILD_DATA:\n",
    "    goodvscorrupt = GoodVSCorrupt()\n",
    "    goodvscorrupt.make_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8088\n"
     ]
    }
   ],
   "source": [
    "training_data = np.load(\"training_data_21_03_20.npy\", allow_pickle=True)\n",
    "print(len(training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "X = torch.Tensor([i[0] for i in training_data]).view(-1, 50, 50)\n",
    "X = X/255.0\n",
    "y = torch.Tensor([i[1] for i in training_data])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2426\n",
      "5662\n",
      "2426\n"
     ]
    }
   ],
   "source": [
    "VAL_PCT = 0.3\n",
    "val_size = int(len(X)*VAL_PCT)\n",
    "print(val_size)\n",
    "\n",
    "train_X = X[:-val_size]\n",
    "train_y = y[:-val_size]\n",
    "\n",
    "test_X = X[-val_size:]\n",
    "test_y = y[-val_size:]\n",
    "\n",
    "print(len(train_X))\n",
    "print(len(test_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This returns the train data accuracy and loss\n",
    "\n",
    "def fwd_pass(X, y, train=False):\n",
    "    # when data pass thru here by default weight will not be updated\n",
    "\n",
    "    if train:\n",
    "        net.zero_grad()\n",
    "    outputs = net(X)\n",
    "    matches  = [torch.argmax(i)==torch.argmax(j) for i, j in zip(outputs, y)]\n",
    "    acc = matches.count(True)/len(matches)\n",
    "    loss = loss_function(outputs, y)\n",
    "\n",
    "    if train:\n",
    "        loss.backward() # backpropagation\n",
    "        optimizer.step()\n",
    "\n",
    "    return acc, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.494 tensor(0.2495, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# This returns the test data accuracy and loss\n",
    "    \n",
    "def test(size=32):\n",
    "    X, y = test_X[:size], test_y[:size]\n",
    "    val_acc, val_loss = fwd_pass(X.view(-1, 1, 50, 50).to(device), y.to(device))\n",
    "    return val_acc, val_loss\n",
    "\n",
    "val_acc, val_loss = test(size=1000)\n",
    "print(val_acc, val_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model-1584809972\n"
     ]
    }
   ],
   "source": [
    "# running the whole thing\n",
    "\n",
    "import time\n",
    "\n",
    "MODEL_NAME = f\"model-{int(time.time())}\"  # gives a dynamic model name, to just help with things getting messy over time. \n",
    "net = Net().to(device)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "print(MODEL_NAME)\n",
    "\n",
    "\n",
    "def train(net):\n",
    "    BATCH_SIZE = 100\n",
    "    EPOCHS = 3 #30\n",
    "\n",
    "    with open(\"model_21_03_20.log\", \"a\") as f:\n",
    "        for epoch in range(EPOCHS):\n",
    "            for i in tqdm(range(0, len(train_X), BATCH_SIZE)):\n",
    "                batch_X = train_X[i:i+BATCH_SIZE].view(-1,1,50,50)\n",
    "                batch_y = train_y[i:i+BATCH_SIZE]\n",
    "\n",
    "                batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "\n",
    "                acc, loss = fwd_pass(batch_X, batch_y, train=True)\n",
    "\n",
    "                #print(f\"Acc: {round(float(acc),2)}  Loss: {round(float(loss),4)}\")\n",
    "                #f.write(f\"{MODEL_NAME},{round(time.time(),3)},train,{round(float(acc),2)},{round(float(loss),4)}\\n\")\n",
    "                # just to show the above working, and then get out:\n",
    "                if i % 50 == 0:\n",
    "                    # for every 50 steps we;ll calc val accuracy and loss\n",
    "                    val_acc, val_loss = test(size=100)\n",
    "                    f.write(f\"{MODEL_NAME},{round(time.time(),3)},{round(float(acc),2)},{round(float(loss), 4)},{round(float(val_acc),2)},{round(float(val_loss),4)},{epoch}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57/57 [00:55<00:00,  1.03it/s]\n",
      "100%|██████████| 57/57 [00:57<00:00,  1.01s/it]\n",
      "100%|██████████| 57/57 [00:57<00:00,  1.01s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train(net)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Comparing test accu, loss vs train accu, loss\n",
    "# plot this on sublime \n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "\n",
    "style.use(\"ggplot\")\n",
    "\n",
    "#model_name = MODEL_NAME #\"model-1570499409\" # grab whichever model name you want here. We could also just reference the MODEL_NAME if you're in a notebook still.\n",
    "\n",
    "model_name = \"model-1584809972\" \n",
    "\n",
    "def create_acc_loss_graph(model_name):\n",
    "    contents = open(\"model_21_03_20.log\", \"r\").read().split(\"\\n\")\n",
    "\n",
    "    times = []\n",
    "    accuracies = []\n",
    "    losses = []\n",
    "\n",
    "    val_accs = []\n",
    "    val_losses = []\n",
    "\n",
    "    for c in contents:\n",
    "        if model_name in c:\n",
    "            name, timestamp, acc, loss, val_acc, val_loss, none = c.split(\",\")\n",
    "\n",
    "            times.append(float(timestamp))\n",
    "            accuracies.append(float(acc))\n",
    "            losses.append(float(loss))\n",
    "\n",
    "            val_accs.append(float(val_acc))\n",
    "            val_losses.append(float(val_loss))\n",
    "\n",
    "\n",
    "    fig = plt.figure()\n",
    "\n",
    "    ax1 = plt.subplot2grid((2,1), (0,0)) #(2,1) 2 by 1 grid, (0,0) this graph will start at the x=0,y=0 mark\n",
    "    ax2 = plt.subplot2grid((2,1), (1,0), sharex=ax1) # (1,0) start at 1,0 mark, and will share x axis with  graph ax1\n",
    "\n",
    "\n",
    "    ax1.plot(times, accuracies, label=\"acc\")\n",
    "    ax1.plot(times, val_accs, label=\"val_acc\")\n",
    "    ax1.legend(loc=2) # location 2\n",
    "    ax2.plot(times,losses, label=\"loss\")\n",
    "    ax2.plot(times,val_losses, label=\"val_loss\")\n",
    "    ax2.legend(loc=2)\n",
    "    plt.savefig(\"model_21_03_20.png\", dpi =800)\n",
    "    #plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving a model\n",
    "\n",
    "torch.save(net.state_dict(),\"/Users/youpele/Documents/WZL/12032020/another1.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading a model\n",
    "\n",
    "model = Net()\n",
    "#model.load_state_dict(\"/Users/youpele/Documents/WZL/12032020/another1.pth\")\n",
    "\n",
    "\n",
    "model.load_state_dict(torch.load(\"/Users/youpele/Documents/WZL/12032020/another1.pth\", map_location='cpu'))\n",
    "#model = torch.load(\"/Users/youpele/Documents/WZL/12032020/another1.pth\", map_location='cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv3): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=5184, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import shutil\n",
    "import torch\n",
    "from torchvision import datasets, models, transforms\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_signal(model_path, containing_folder, image_folder, image_index = 1):\n",
    "    \n",
    "    # Setting up folders\n",
    "    folders = ['predicted_corrupt', 'predicted_good' ]\n",
    "    \n",
    "    for folder in folders:\n",
    "        new_folder = containing_folder + folder\n",
    "        if not os.path.exists(new_folder):\n",
    "            os.makedirs(new_folder)\n",
    "            print(\"Folder \" , new_folder ,  \" has been created \")\n",
    "        else: \n",
    "            print(\"Directory \" , new_folder ,  \" already exists\")\n",
    "            \n",
    "            \n",
    "    \n",
    "    # Prediction and copying predicted image to its respective folder\n",
    "    \n",
    "    model = Net()\n",
    "    model.load_state_dict(torch.load(model_path, map_location='cpu'))\n",
    "    \n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    pred_s = []\n",
    "    tf = transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    \n",
    "    image_datasets = datasets.ImageFolder(containing_folder, tf)\n",
    "    inputs = image_datasets[image_index]\n",
    "    print (inputs[1])\n",
    "    with torch.no_grad():\n",
    "        inputs = inputs[0][None, ...].to(device)\n",
    "        outputs = model(inputs)\n",
    "        pred_s.append(outputs.argmax(dim=1).cpu().item())\n",
    "    print(pred_s[0])\n",
    "    \n",
    "    \n",
    "    image_folder_list = os.listdir(image_folder)\n",
    "    if pred_s[0] == 0:\n",
    "        print('This signal is corrupted!')\n",
    "        \n",
    "        shutil.copy(image_folder + image_folder_list[image_index],\n",
    "                        containing_folder + folders[0] + '/' + image_folder_list[image_index])\n",
    "        print('Image has been moved to', folders[0], ' folder.')\n",
    "        \n",
    "    elif pred_s[0] == 1:\n",
    "        \n",
    "        print('This signal is good!')\n",
    "        shutil.copy(image_folder + image_folder_list[image_index],\n",
    "                        containing_folder + folders[1] + '/' + image_folder_list[image_index])\n",
    "        print('Image has been moved to', folders[1], 'folder.')\n",
    "        \n",
    "        \n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# multiple image prediction\n",
    "\n",
    "containing_folder = '/Users/youpele/Documents/WZL/12032020/dataset/unused/'\n",
    "\n",
    "image_folder = '/Users/youpele/Documents/WZL/12032020/dataset/unused/files/'\n",
    "\n",
    "model_path = \"/Users/youpele/Documents/WZL/12032020/another1.pth\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory  /Users/youpele/Documents/WZL/12032020/dataset/unused/predicted_corrupt  already exists\n",
      "Directory  /Users/youpele/Documents/WZL/12032020/dataset/unused/predicted_good  already exists\n",
      "0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size 32 1 5 5, expected input[1, 3, 224, 224] to have 1 channels, but got 3 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-4a67f89a7ad2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m                               \u001b[0mcontaining_folder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontaining_folder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                               \u001b[0mimage_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                               image_folder=image_folder)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-947739b2fd18>\u001b[0m in \u001b[0;36mpredict_signal\u001b[0;34m(model_path, containing_folder, image_folder, image_index)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mpred_s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_s\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-71fc3a606950>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_linear\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#we flatten\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-71fc3a606950>\u001b[0m in \u001b[0;36mconvs\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconvs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0;31m# 2 by 2, is the shape of the pooling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mconv2d_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    340\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    341\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 342\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size 32 1 5 5, expected input[1, 3, 224, 224] to have 1 channels, but got 3 channels instead"
     ]
    }
   ],
   "source": [
    "\n",
    "for image_index in range(0, len(os.listdir(image_folder))):\n",
    "    \n",
    "    b = predict_signal(model_path=model_path,\n",
    "                              containing_folder=containing_folder,\n",
    "                              image_index=image_index, \n",
    "                              image_folder=image_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I always get errors here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Net' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-58e7ea0d2d2f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m#model = squeezenet1_1(pretrained=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Net' is not defined"
     ]
    }
   ],
   "source": [
    "from torchvision.models import squeezenet1_1\n",
    "\n",
    "\n",
    "model = Net(num_classes=2)\n",
    "#model = squeezenet1_1(pretrained=True)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(image_path):\n",
    "    print(\"Prediction in progress\")\n",
    "    image = Image.open(image_path)\n",
    "\n",
    "    # Define transformations for the image, should (note that imagenet models are trained with image size 50)\n",
    "    transformation = transforms.Compose([\n",
    "        transforms.CenterCrop(50),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "\n",
    "    ])\n",
    "\n",
    "    # Preprocess the image\n",
    "    image_tensor = transformation(image).float()\n",
    "\n",
    "    # Add an extra batch dimension since pytorch treats all images as batches\n",
    "    image_tensor = image_tensor.unsqueeze_(0)\n",
    "    \n",
    "    input1 = transforms.ToPILImage()(image_tensor.float())\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    #if torch.cuda.is_available():\n",
    "        #image_tensor.cuda()\n",
    "\n",
    "    # Turn the input into a Variable\n",
    "    #input = Variable(image_tensor)\n",
    "    input = Variable(input1)\n",
    "\n",
    "    # Predict the class of the image\n",
    "    output = model(input)\n",
    "\n",
    "    index = output.data.numpy().argmax()\n",
    "\n",
    "    return index\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction in progress\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "img should be PIL Image. Got <class 'torch.Tensor'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-144-787207d9b5b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/Users/youpele/Documents/WZL/12032020/dataset/unused/corrupted/R3_1_Stempel_hinten_132.png\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclass_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Predicted Class \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-143-c574c276b5c7>\u001b[0m in \u001b[0;36mpredict_image\u001b[0;34m(image_path)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# Preprocess the image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mimage_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# Add an extra batch dimension since pytorch treats all images as batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m   1172\u001b[0m             \u001b[0mPIL\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRandomly\u001b[0m \u001b[0mgrayscaled\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1173\u001b[0m         \"\"\"\n\u001b[0;32m-> 1174\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_grayscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_output_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_output_channels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_grayscale\u001b[0;34m(img, num_output_channels)\u001b[0m\n\u001b[1;32m    831\u001b[0m     \"\"\"\n\u001b[1;32m    832\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_is_pil_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'img should be PIL Image. Got {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnum_output_channels\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: img should be PIL Image. Got <class 'torch.Tensor'>"
     ]
    }
   ],
   "source": [
    "index = predict_image(\"/Users/youpele/Documents/WZL/12032020/dataset/unused/corrupted/R3_1_Stempel_hinten_132.png\")\n",
    "\n",
    "prediction = class_map[str(index)][1]\n",
    "\n",
    "print(\"Predicted Class \", prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "img = cv2.imread(\"/Users/youpele/Documents/WZL/12032020/dataset/unused/corrupted/R1_1_Stempel_hinten_1743.png\")\n",
    "\n",
    "# You may need to convert the color.\n",
    "#img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "im_pil = Image.fromarray(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "pil2tensor = transforms.ToTensor()\n",
    "tensor2pil = transforms.ToPILImage()\n",
    "\n",
    "# Read the image from file. Assuming it is in the same directory.\n",
    "pil_image = Image.open(\"/Users/youpele/Documents/WZL/12032020/dataset/unused/corrupted/R1_1_Stempel_hinten_1743.png\")\n",
    "rgb_image = pil2tensor(pil_image)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(rgb_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output = model(rgb_image[50,50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected 4-dimensional input for 4-dimensional weight 32 1 5 5, but got 3-dimensional input of size [4, 2000, 3000] instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-100-7e9c53f9813f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#reshaped = rgb_image.permute(2, 0, 1).unsqueeze(0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0myour_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrgb_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-71fc3a606950>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_linear\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#we flatten\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-71fc3a606950>\u001b[0m in \u001b[0;36mconvs\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconvs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0;31m# 2 by 2, is the shape of the pooling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mconv2d_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    340\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    341\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 342\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected 4-dimensional input for 4-dimensional weight 32 1 5 5, but got 3-dimensional input of size [4, 2000, 3000] instead"
     ]
    }
   ],
   "source": [
    "#reshaped = rgb_image.permute(2, 0, 1).unsqueeze(0)\n",
    "your_result = model(rgb_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv3): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=5184, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import needed packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.transforms import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "from torchvision.models import squeezenet1_1\n",
    "import torch.functional as F\n",
    "import requests\n",
    "import shutil\n",
    "from io import open\n",
    "import os\n",
    "from PIL import Image\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(image_path):\n",
    "    print(\"Prediction in progress\")\n",
    "    image = Image.open(image_path)\n",
    "\n",
    "    # Define transformations for the image, should (note that imagenet models are trained with image size 50)\n",
    "    transformation = transforms.Compose([\n",
    "        transforms.CenterCrop(50),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "\n",
    "    ])\n",
    "\n",
    "    # Preprocess the image\n",
    "    image_tensor = transformation(image).float()\n",
    "\n",
    "    # Add an extra batch dimension since pytorch treats all images as batches\n",
    "    image_tensor = image_tensor.unsqueeze_(0)\n",
    "    \n",
    "    input1 = transforms.ToPILImage()(image_tensor.float())\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    #if torch.cuda.is_available():\n",
    "        #image_tensor.cuda()\n",
    "\n",
    "    # Turn the input into a Variable\n",
    "    #input = Variable(image_tensor)\n",
    "    input = Variable(input1)\n",
    "\n",
    "    # Predict the class of the image\n",
    "    output = model(input)\n",
    "\n",
    "    index = output.data.numpy().argmax()\n",
    "\n",
    "    return index\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(image_path):\n",
    "    print(\"Prediction in progress\")\n",
    "    image = Image.open(image_path)\n",
    "\n",
    "    # Define transformations for the image, should (note that imagenet models are trained with image size 50)\n",
    "    transformation = transforms.Compose([\n",
    "        transforms.CenterCrop(50),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "\n",
    "    ])\n",
    "\n",
    "        # Preprocess the image\n",
    "    image_tensor = transformation(image).float()\n",
    "\n",
    "    # Add an extra batch dimension since pytorch treats all images as batches\n",
    "    #image_tensor = image_tensor.unsqueeze_(0)\n",
    "    \n",
    "    image_PIL =transforms.ToPILImage()\n",
    "    \n",
    "    \n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        image_tensor.cuda()\n",
    "\n",
    "    # Turn the input into a Variable\n",
    "    input = Variable(image_PIL(image_tensor))\n",
    "\n",
    "    # Predict the class of the image\n",
    "    output = model(input)\n",
    "\n",
    "    index = output.data.numpy().argmax()\n",
    "\n",
    "    return index\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction in progress\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "img should be PIL Image. Got <class 'torch.Tensor'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-171-7cc3feaea9ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minput1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/Users/youpele/Documents/WZL/12032020/dataset/unused/corrupted/R3_1_Stempel_hinten_132.png\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-170-600aaee389a8>\u001b[0m in \u001b[0;36mpredict_image\u001b[0;34m(image_path)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# Preprocess the image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mimage_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# Add an extra batch dimension since pytorch treats all images as batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m   1172\u001b[0m             \u001b[0mPIL\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRandomly\u001b[0m \u001b[0mgrayscaled\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1173\u001b[0m         \"\"\"\n\u001b[0;32m-> 1174\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_grayscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_output_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_output_channels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_grayscale\u001b[0;34m(img, num_output_channels)\u001b[0m\n\u001b[1;32m    831\u001b[0m     \"\"\"\n\u001b[1;32m    832\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_is_pil_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'img should be PIL Image. Got {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnum_output_channels\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: img should be PIL Image. Got <class 'torch.Tensor'>"
     ]
    }
   ],
   "source": [
    "input1 = predict_image(\"/Users/youpele/Documents/WZL/12032020/dataset/unused/corrupted/R3_1_Stempel_hinten_132.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_tensor = transforms.ToTensor()\n",
    "img_pil = transforms.ToPILImage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "array = np.asarray(\"/Users/youpele/Documents/WZL/12032020/dataset/unused/corrupted/R3_1_Stempel_hinten_132.png\")\n",
    "type(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "pic should be 2/3 dimensional. Got 0 dimensions.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-179-6ac45e9e606b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_pil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \"\"\"\n\u001b[0;32m--> 136\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pil_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_pil_image\u001b[0;34m(pic, mode)\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pic should be 2/3 dimensional. Got {} dimensions.'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: pic should be 2/3 dimensional. Got 0 dimensions."
     ]
    }
   ],
   "source": [
    "\n",
    "a = img_pil(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
